{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cotiss Takehome Project\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Australia and New Zealand alone there are over 40,000 government contracts posted each year. Most contracts have common meta data such as the date they are published, their associated category and the region in which the good/service is being procured. In addition, they contain free text that describes the details of the tender.\n",
    "\n",
    "At Cotiss, part of our mission is to help businesses find those opportunities that are relevant for them. In order to make these contracts accessible to SMEs, it should take minimal resources to find relevant opportunities. One of the key components in this is making them searchable. This means that the content of the tender can be processed, so relevant businesses can be notified.\n",
    "\n",
    "A key issue in searching for listings is that, often, those searching don't know what phrases to search for. An improvement for plaintext search is an autocomplete search feature that prompts the user on what they should search. You have probably experienced this when doing a Google search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The task\n",
    "\n",
    "For this task, we have collated a file called _listings.json_ containing the content of thousands of public tenders posted through Government portals. Your job is to transform the data into a structure that is easier to search.\n",
    "\n",
    "We need to improve our capabilities for our users to quickly locate tenders that matter to them. Currently our users are having a hard time to filter out irrelevant tenders and the locating tenders for their business.\n",
    "\n",
    "User story: As a user I want to be able to effectively search for tenders that relate to my business\n",
    "Acceptance Criteria\n",
    "- Can efficiently search thousands of tenders\n",
    "- Only shows tenders relating to the search phrase\n",
    "- Recommends what phrases to search for\n",
    "- Filters results by meta data\n",
    "\n",
    "We are not asking you to implement the search component itself. The task is to create a model that will make it simpler for a search API to perform the above tasks. This project is open ended and designed to provide a platform to show your creativity, coding and problem solving skills. There is also no strict requirement on what the final solution should look like. There are no right or wrong answers so don't overthink it - just state your assumptions and justification for the model you think works best. If you have any questions, please reach out to matthew.oh@cotiss.com and ask away - we're here to help!\n",
    "\n",
    "Your approach may also include external services in addition to python. However, if you do so, please state what service was used and how you used it. An example might be using AWS Comprehend to extract key phrases. There are also many examples for this type of application online. If you use example code, please include the required attribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output\n",
    "\n",
    "- Once the model has been produced, it should show the output when run on the file provided. \n",
    "- Graphical analysis in order to interprets the results.\n",
    "- Suggestions on how this data could be combined with the search input to produce results. For example: \"We recommend using n-gram to match a searched phrase, the data returned will be a list of matched tenders and relevant meta-data\".\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission\n",
    "\n",
    "Before you start this project, please fork the repository and add it to your GitHub. Everything must be documented below this description, including the code. From the time you have received the project, you will have 7 days to complete it. Once you have completed it, reach out to matthew.oh@cotiss.com. We will aim to get back to you within a couple of days. Last of all, please timebox this excerice to 2-3 hours. We value your time, so don't expect a near perfect solution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Starting work on project\n",
    "\n",
    "Start time: 10:06am"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "#Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Short EDA to get familiar with the data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Read in the data as a dataframe\n",
    "data = pd.read_json(\"listings.json\")\n",
    "\n",
    "#Check data characterstics\n",
    "print(\"Number of listsings: \", len(data))\n",
    "print(\"Column names: \", data.columns)\n",
    "\n",
    "#Print out some random listings to get a feel of what's going on and type of text we have\n",
    "for i in range(15):\n",
    "    #Get random index\n",
    "    index = random.randint(0, len(data))\n",
    "    print()\n",
    "    print(data.id[index])\n",
    "    print()\n",
    "    print(data.title[index])\n",
    "    print()\n",
    "    print(data.blurb[index])\n",
    "    print()\n",
    "    print(data.description[index])\n",
    "    print()\n",
    "    print(data.regions[index])\n",
    "    print(\"==============\")\n",
    "\n",
    "#Seems like title and description might be easiest to work with for now. Blurb contains useful meta-information but needs a lot of cleaning, also some cases where \n",
    "#there are no blurbs (empty strings). Some cases of description and blurb being duplicates too."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning up the titles and descriptions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Cleaning the text data\n",
    "def preprocess_text(text):\n",
    "\n",
    "    #keep only alphanumeric characters and lower text\n",
    "    text_no_punct = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower()\n",
    "    \n",
    "    #Lemmatize, keep only open-class POS-words (maybe too much..) and numbers and remove stopwords\n",
    "    #Lemmatization will introduce uniformity to the data\n",
    "    #Closed-class words (determinters, conjunctions) tend to not hold too much useful information and might make our structures too sparse\n",
    "    #TODO: Consider adding more tags\n",
    "    #List of tags: https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "    closed_classes = ['CC', 'DT', 'IN', 'PRP', 'PRP$', 'WDT', 'WP', 'WP$', \"LS\", \"MD\", \"PDT\", \"FW\", \"TO\"]\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "    text_pos = nltk.pos_tag(text_no_punct.split())\n",
    "    open_class_text = [pos_tuple[0] for pos_tuple in text_pos if pos_tuple[1] not in closed_classes and pos_tuple[0] not in stopword_list]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in open_class_text]\n",
    "    \n",
    "    #Return cleaned string\n",
    "    return \" \".join(lemmatized_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Form new columns in the DF for the cleaned title and descriptions which will be used for the proposed approaches\n",
    "data['cleaned_title'] = data.title.apply(lambda x: preprocess_text(x))\n",
    "data['cleaned_description'] = data.description.apply(lambda x: preprocess_text(x))\n",
    "\n",
    "#Concat the two together to increase likelihood of retrieval\n",
    "#In some cases this could help if title or description contains information the other one doesn't\n",
    "#Could be redundant too, as there are also duplicate cases\n",
    "data['clean_title_description'] = data['cleaned_title'] + \" \" + data['cleaned_description']\n",
    "\n",
    "#Double check things are looking correct\n",
    "print(data['clean_title_description'].head(10))\n",
    "print(len(data))\n",
    "\n",
    "#TODO: display some of the most commonly occuring words so that I can form better search phrases for the testing. Not too familiar with the context of tendering and what services are typically offered"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    monitoring regulatory change reserve bank seek...\n",
      "1    provision interpretation translation service a...\n",
      "2    lao australia institute phase iii lai iii dfat...\n",
      "3    raaf richmond cctv upgrade nsw scope work proj...\n",
      "4    management operation catering cafeteria servic...\n",
      "5    raaf townsville sewer water work qld maintenan...\n",
      "6    global lightning data service procurement aim ...\n",
      "7    angle park 14 dwelling construction defence ho...\n",
      "8    atm 20 1280 expeditioner training service depa...\n",
      "9    national priority 4 5 residual current device ...\n",
      "Name: clean_title_description, dtype: object\n",
      "1982\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 1\n",
    "## Set up a dictionary of n_grams\n",
    "\n",
    "A dictionary where each key is an n-gram from the data and the values are indices of the listings it is found in. \n",
    "\n",
    "The search process would then take the input phrase from the user, clean it up as well and check which keys appear in it. Get a full list of the listing indices which hold that key, and then get the most frequently appearing listings.\n",
    "\n",
    "Using the cleaned up text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Make a new column with chacter level n_grams for the cleaned descriptions\n",
    "data['text_ngrams'] = data.clean_title_description.apply(lambda x: [\"\".join(gram) for gram in list(ngrams(x,n=5))])\n",
    "\n",
    "#Function for making the dictionary structure\n",
    "def form_dictionary(dataset):\n",
    "    #Initialize the dictionary\n",
    "    n_grams_dict = {}\n",
    "\n",
    "    #Iterate over the given dataset\n",
    "    for index in tqdm(range(len(data))):\n",
    "\n",
    "        #Get the row of grams\n",
    "        row_grams = data['text_ngrams'][index]\n",
    "\n",
    "        #Iterate over the grams to add to dictionary\n",
    "        for gram in row_grams:\n",
    "            \n",
    "            #Check if gram already in dict, if not initiate list for indexes and add index\n",
    "            if gram not in n_grams_dict:\n",
    "                n_grams_dict[gram] = []\n",
    "                n_grams_dict[gram].append(index)\n",
    "\n",
    "            else:\n",
    "                n_grams_dict[gram].append(index)\n",
    "\n",
    "    #Finally, make the values sets\n",
    "    for key, value in n_grams_dict.items():\n",
    "        n_grams_dict[key] = set(value)\n",
    "    \n",
    "    return n_grams_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Get the dictionary structure\n",
    "n_grams_dict = form_dictionary(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1982/1982 [00:00<00:00, 3799.67it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "#To test out how it works on some search phrases\n",
    "#Give the search phrase, the dictionary structure and the top N matching listings\n",
    "def search_grams_dict(search_phrase, n_grams_dict, n_results):\n",
    "\n",
    "    #Clean the phrase too\n",
    "    clean_search_phrase = preprocess_text(search_phrase)\n",
    "\n",
    "    #Get the keys as list\n",
    "    keys_list = n_grams_dict.keys()\n",
    "\n",
    "    #Now go through the keys and if they are in the word, store indicies\n",
    "    indices = []\n",
    "    for key in keys_list:\n",
    "        if key in clean_search_phrase:\n",
    "            indices.append(list(n_grams_dict[key]))\n",
    "    \n",
    "    #flatten list..\n",
    "    flat_indices_list = [item for sublist in indices for item in sublist]\n",
    "    \n",
    "    #Final step, return the n_results most frequent indicies, and thus the original entries from the data\n",
    "    #TODO: return also the counts so can display for analysis\n",
    "    counts = Counter(flat_indices_list)\n",
    "    results = counts.most_common(n_results)\n",
    "    result_indices = [result[0] for result in results]\n",
    "    listing_frequencies = [result[1] for result in results]\n",
    "\n",
    "    #Return the indices corresponding to the listings of the data\n",
    "    return result_indices, listing_frequencies"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "#Funciton for displaying the results as well as some analysis\n",
    "def display_results(data_indices, data, search_phrase, listing_frequencies):\n",
    "\n",
    "    search_phrase = preprocess_text(search_phrase)\n",
    "    \n",
    "    #Iterate over the indices and pull from data\n",
    "    listing_counter = 0\n",
    "\n",
    "    for index in data_indices:\n",
    "        print(f\"Listing frequency: {listing_frequencies[listing_counter]}\")\n",
    "        print(\"Title: \" + data.title[index] + \"\\n\")\n",
    "        print(\"Description: \" + data.description[index] + \"...\" + \"\\n\")\n",
    "\n",
    "        #Check for each word in search phrase how many times it appears in the title and description of the found listting\n",
    "        text_list = data['clean_title_description'][index].lower().split()\n",
    "        print(\"Word frequencies: \")\n",
    "        for word in search_phrase.split():\n",
    "            print(f\"{word} ({text_list.count(word)})\")\n",
    "        \n",
    "        print(\"\\n ======================= \\n\")\n",
    "\n",
    "        listing_counter += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Define some search phrase\n",
    "search_phrase = \"building construction\"\n",
    "\n",
    "#Get indices and listing frequencies from the dicionary\n",
    "data_indices, listing_frequencies = search_grams_dict(search_phrase, n_grams_dict, 5)\n",
    "\n",
    "#Display some results\n",
    "display_results(data_indices, data, search_phrase, listing_frequencies)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some notes on how I think the method has worked\n",
    "\n",
    "While it does find relevant listings, it is a bit hit or miss, with the results highly dependant on the search phrase used. The search phrases need to be words which are already contained in the data, out-of-vocab words will not be discovered. The method works well with words which are \"related\" to each other, as in, they are more likely to be mentioned together (such as \"building construction\", \"highway work\"). If a user search is a bit more unique, let's say \"environmental vacuum\" the method right now tends to favour one word over the other. Listings with the word \"vacuum\" tend to be ignored as it is a less common word in the corpus.\n",
    "\n",
    "The word frequencies are not that related to the listing frequency because the values (indices) in the dictionary structure were made sets in order to reduce the effect of some strange descriptions which had a lot of duplicating text.\n",
    "\n",
    "The nice thing about this method is that it is quick, and if a user happens to misspell a word in the phrase, lets say \"highway word\" this should be robust as we are looking at character level n-grams, rather than full words.\n",
    "\n",
    "## Food for thought to come back to if I have some time after implementing second approach\n",
    "* Add a way to match and prioritise locations if they are put in the search phrase too, using the regions provided \n",
    "* Improve the word count (\"environment\" is not counted if \"environmental\" in text, even though it is taken into account for the actual listing rankings due to the character n-grams)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ============================================================"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 2\n",
    "\n",
    "It would be nice if there is a way for the user to input anything they want and base the retrieval on semantic similarity. So the idea here would be to form word/sentence embeddings of the titles and descriptions of the listings using some language model (BERT for example). \n",
    "\n",
    "Then, a search phrase is also transformed into an embedding, and using a distance metric such as cosine similarity, pull the top X most related listings. Check both the title and description and return based on that (if both the same title and description are ranked high against the search phrase, prioritise that listing). At the same time, create a TF-IDF matrix of all descriptions of listings. Using these, for each returned listing, the user can be suggested better search phrases by listing the words of each description with the highest TF-IDF scores.\n",
    "\n",
    "Downside here would probably be speed, as you need to compute embeddings of the search phrase. The dimensions of BERT are also large (760+) but consider reducing these using PCA for example.\n",
    "\n",
    "Another limitation might be the token limit placed by some language models, especially in the cases of some descriptions which are really long. But an assumption here can be made that the most important and descriptive information of a description is contained in the first few sentences.\n",
    "\n",
    "Again, work on the preprocessed text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "def create_embeddings(text_list, model):\n",
    "\n",
    "    #Store the embeddings in a list\n",
    "    sentence_embeddings = []\n",
    "    #Iterate over the list\n",
    "    for text in tqdm(text_list):\n",
    "        text_embedding = model.encode(text)\n",
    "        sentence_embeddings.append(text_embedding)\n",
    "\n",
    "    return sentence_embeddings\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#create two models\n",
    "#first one is a word embedding model\n",
    "#Using a language model from the HuggingFace library. Model here can be changed depending on the task and language\n",
    "#When using for the first time I believe the model needs to be downloaded which is also a time consuming step\n",
    "word_embedding_model = models.Transformer('sentence-transformers/all-mpnet-base-v2', max_seq_length=256)\n",
    "#The second is a pooling model which will take individual word embeddings and pool them together to form sentence embeddings\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "#Put them together\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "#Create separate lists for title and description embeddings\n",
    "#This method takes some time (about 2mins for titles and 7mins for descriptions.. too long)\n",
    "title_embeddings = create_embeddings(data['cleaned_title'].to_list(), model)\n",
    "print(len(title_embeddings))\n",
    "description_embeddings = create_embeddings(data['cleaned_description'].to_list(), model)\n",
    "print(len(description_embeddings))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "#TODO: Come back to this, didn't get dimensions matching for the search input\n",
    "# #Lets reduce the embeddings using PCA (80% variance)\n",
    "# pca = PCA(.80).fit(title_embeddings)\n",
    "# title_embeddings_pca = pca.transform(title_embeddings)\n",
    "# print(\"Titles reduced down to: \", len(title_embeddings_pca[0]), \" dimensions\")\n",
    "\n",
    "# #Do the same for the descriptions\n",
    "# # pca = PCA(.80).fit(description_embeddings)\n",
    "# description_embeddings_pca = pca.transform(description_embeddings)\n",
    "# print(\"Descriptions reduced down to: \", len(description_embeddings_pca[0]), \" dimensions\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Titles reduced down to:  119  dimensions\n",
      "Descriptions reduced down to:  119  dimensions\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "#Next, let's form a list of tuples, for both embeddings\n",
    "#Index in list refers to index of original listing\n",
    "embedding_tuples = list(zip(title_embeddings, description_embeddings))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "#Function for getting X most similar listings based on title and description embeddings\n",
    "def get_listings(embedding_tuples, top_n, search_phrase_embedding, data):\n",
    "    \n",
    "    #make some structures to hold your results\n",
    "    index_count = 0\n",
    "    similarities_titles = []\n",
    "    similarities_descriptions = []\n",
    "\n",
    "    #Iterate over all embeddings\n",
    "    for embedding in embedding_tuples:\n",
    "\n",
    "        #Get cosine similarity with title\n",
    "        cosine_title = cosine_similarity(search_phrase_embedding.reshape(1, -1), embedding[0].reshape(1, -1))\n",
    "        similarities_titles.append([cosine_title[0][0], index_count])\n",
    "\n",
    "        #Get cosine similarity with title\n",
    "        cosine_description = cosine_similarity(search_phrase_embedding.reshape(1, -1), embedding[1].reshape(1, -1))\n",
    "        similarities_descriptions.append([cosine_description[0][0], index_count])\n",
    "\n",
    "        index_count += 1\n",
    "\n",
    "    #now that we have both lists, order them based on their scores and take the top X\n",
    "    titles_sorted = sorted(similarities_titles, key=lambda x: x[0], reverse= True)[0:top_n]\n",
    "    descriptions_sorted = sorted(similarities_descriptions, key=lambda x: x[0], reverse= True)[0:top_n]\n",
    "\n",
    "    #extract indices only\n",
    "    indicies_titles = [titles[1] for titles in titles_sorted]\n",
    "    indices_descriptions = [descriptions[1] for descriptions in descriptions_sorted]\n",
    "\n",
    "    all_indices = indicies_titles + indices_descriptions\n",
    "\n",
    "    #Count again\n",
    "    counts = Counter(all_indices)\n",
    "    results = counts.most_common(top_n)\n",
    "\n",
    "    final_indices = [result[0] for result in results]\n",
    "    \n",
    "    #Print the resulting entries\n",
    "    for index in final_indices:\n",
    "\n",
    "        print(\"Title: \" + data.title[index] + \"\\n\")\n",
    "        print(\"Description: \" + data.description[index] + \"...\" + \"\\n\")\n",
    "        \n",
    "        print(\"\\n ======================= \\n\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "search_phrase = \"I specialise in educational construction\"\n",
    "\n",
    "#Encode the search phrase too with the language model\n",
    "search_phrase_embedding = model.encode(search_phrase)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_listings(embedding_tuples, 5, search_phrase_embedding, data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quick notes on how I believe the second method performs\n",
    "\n",
    "Setting up of the second approach certainly takes more time, and I am not sure how this would work in an online setting where embeddings need to be computed on the go. \n",
    "\n",
    "However, once the embeddings have been computed and the language model is initalized, getting the search phrase embedding is still fast and the suggested listings work surprisingly well considering no dimensionality reduction has been applied.\n",
    "\n",
    "This approach gives the user a lot more freedom in their inputs. I played around with several wordings of work related to working on educational/school facilities and most suggestions are related to the search.\n",
    "\n",
    "This is an advantage to the n-grams approach where wording and keywords are predefined in the dictionary and using search terms outside of that would result in poor listing selection.\n",
    "\n",
    "Further, as multi-lingual models exist, this should allow for various languages to be used for searching, while still producing relevant listings. \n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}